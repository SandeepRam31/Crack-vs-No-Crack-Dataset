{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e0d1daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D, Dropout, Input,\\\n",
    "Flatten, Dense, MaxPool2D\n",
    "import os\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f661e738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'code_files',\n",
       " 'Crack detection.ipynb',\n",
       " 'images',\n",
       " 'predict',\n",
       " 'Results']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eccacbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential([\n",
    "#     Input(shape = (227, 227, 3)),\n",
    "#     Conv2D(32, 3, padding = 'same', strides = 1, activation = 'relu'),\n",
    "#     BatchNormalization(),\n",
    "#     MaxPooling2D(),\n",
    "#     Dropout(0.5),\n",
    "#     Conv2D(64, 3, padding = 'same', strides = 1, activation = 'relu'),\n",
    "#     BatchNormalization(),\n",
    "#     MaxPooling2D(),\n",
    "#     Dropout(0.5),\n",
    "#     Conv2D(128, 3, padding = 'same', strides = 1, activation = 'relu'),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.5),\n",
    "#     GlobalAveragePooling2D(),\n",
    "#     Dense(64, activation = 'relu'),\n",
    "#     Dense(1, activation = 'softmax')\n",
    "# ])\n",
    "\n",
    "# model.compile(\n",
    "#     metrics = ['accuracy'],\n",
    "#     optimizer = 'adam',\n",
    "#     loss = 'BinaryCrossentropy'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95cec3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(227,227,3))\n",
    "x = Conv2D(16,3,activation=\"relu\")(input_layer)\n",
    "x = MaxPool2D((2,2))(x)\n",
    "\n",
    "x = Conv2D(32,3,activation=\"relu\")(x)\n",
    "x = MaxPool2D((2,2))(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "output = Dense(1,activation=\"sigmoid\")(x)\n",
    "\n",
    "model=tf.keras.Model(inputs = [input_layer], outputs = [output])\n",
    "\n",
    "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27fee539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 227, 227, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 225, 225, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 110, 110, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 55, 55, 32)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 5,121\n",
      "Trainable params: 5,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9673a900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40000 files belonging to 2 classes.\n",
      "Using 36000 files for training.\n",
      "Found 40000 files belonging to 2 classes.\n",
      "Using 4000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            'images',\n",
    "            labels = 'inferred',\n",
    "            label_mode = 'binary',\n",
    "            color_mode = 'rgb',\n",
    "            batch_size = 32,\n",
    "            shuffle = True,\n",
    "            image_size = (227, 227),\n",
    "            validation_split = 0.1,\n",
    "            subset = 'training',\n",
    "            seed = 123,\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            'images',\n",
    "            labels = 'inferred',\n",
    "            label_mode = 'binary',\n",
    "            color_mode = 'rgb',\n",
    "            batch_size = 32,\n",
    "            shuffle = True,\n",
    "            image_size = (227, 227),\n",
    "            validation_split = 0.1,\n",
    "            subset = 'validation',\n",
    "            seed = 123,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89dd880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    image = x/ 255.0\n",
    "    return image, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "926c1e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = train_ds.map(preprocess), val_ds.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03438509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1125/1125 [==============================] - 79s 67ms/step - loss: 0.5802 - accuracy: 0.6860 - val_loss: 0.2296 - val_accuracy: 0.9507\n",
      "INFO:tensorflow:Assets written to: model_saves\\assets\n",
      "Epoch 2/10\n",
      "1125/1125 [==============================] - 66s 58ms/step - loss: 0.2142 - accuracy: 0.9408 - val_loss: 0.1820 - val_accuracy: 0.9420\n",
      "INFO:tensorflow:Assets written to: model_saves\\assets\n",
      "Epoch 3/10\n",
      "1125/1125 [==============================] - 67s 59ms/step - loss: 0.1646 - accuracy: 0.9561 - val_loss: 0.1396 - val_accuracy: 0.9620\n",
      "INFO:tensorflow:Assets written to: model_saves\\assets\n",
      "Epoch 4/10\n",
      "1125/1125 [==============================] - 69s 62ms/step - loss: 0.1329 - accuracy: 0.9639 - val_loss: 0.1255 - val_accuracy: 0.9592\n",
      "INFO:tensorflow:Assets written to: model_saves\\assets\n",
      "Epoch 5/10\n",
      "1125/1125 [==============================] - 67s 59ms/step - loss: 0.1162 - accuracy: 0.9657 - val_loss: 0.1222 - val_accuracy: 0.9528\n",
      "INFO:tensorflow:Assets written to: model_saves\\assets\n",
      "Epoch 6/10\n",
      "1125/1125 [==============================] - 67s 59ms/step - loss: 0.1024 - accuracy: 0.9697 - val_loss: 0.1052 - val_accuracy: 0.9670\n",
      "INFO:tensorflow:Assets written to: model_saves\\assets\n",
      "Epoch 7/10\n",
      "1125/1125 [==============================] - 71s 63ms/step - loss: 0.1015 - accuracy: 0.9702 - val_loss: 0.0954 - val_accuracy: 0.9693\n",
      "INFO:tensorflow:Assets written to: model_saves\\assets\n",
      "Epoch 8/10\n",
      "1125/1125 [==============================] - 67s 60ms/step - loss: 0.0919 - accuracy: 0.9721 - val_loss: 0.0929 - val_accuracy: 0.9697\n",
      "INFO:tensorflow:Assets written to: model_saves\\assets\n",
      "Epoch 9/10\n",
      "1125/1125 [==============================] - 68s 61ms/step - loss: 0.0880 - accuracy: 0.9732 - val_loss: 0.0904 - val_accuracy: 0.9768\n",
      "INFO:tensorflow:Assets written to: model_saves\\assets\n",
      "Epoch 10/10\n",
      "1125/1125 [==============================] - 73s 65ms/step - loss: 0.0903 - accuracy: 0.9730 - val_loss: 0.0947 - val_accuracy: 0.9680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e915c98fd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs = 10, verbose = 1, validation_data = val_ds,\n",
    "         callbacks = [tf.keras.callbacks.ModelCheckpoint('model_saves', 'val_loss', save_best_only = True,),\n",
    "                     tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                    min_delta=0,\n",
    "                                                    patience=1,\n",
    "                                                    verbose=0,\n",
    "                                                    mode=\"auto\",\n",
    "                                                    baseline=None,\n",
    "                                                    restore_best_weights=True,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d614509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 5s 38ms/step - loss: 0.0904 - accuracy: 0.9768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0904291644692421, 0.9767500162124634]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d48211",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = tf.keras.models.load_model(f'code_files/neural network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc279a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cbd22f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 5s 37ms/step - loss: 0.0904 - accuracy: 0.9768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09042918682098389, 0.9767500162124634]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b670313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    def __init__(self, model, classIdx, layerName=None):\n",
    "        # store the model, the class index used to measure the class\n",
    "        # activation map, and the layer to be used when visualizing\n",
    "        # the class activation map\n",
    "        self.model = model\n",
    "        self.classIdx = classIdx\n",
    "        self.layerName = layerName\n",
    "\n",
    "        # if the layer name is None, attempt to automatically find\n",
    "        # the target output layer\n",
    "        if self.layerName is None:\n",
    "            self.layerName = self.find_target_layer()\n",
    "\n",
    "    def find_target_layer(self):\n",
    "        # attempt to find the final convolutional layer in the network\n",
    "        # by looping over the layers of the network in reverse order\n",
    "        for layer in reversed(self.model.layers):\n",
    "            # check to see if the layer has a 4D output\n",
    "            if len(layer.output.shape) == 4:\n",
    "                return layer.name\n",
    "\n",
    "        # otherwise, we could not find a 4D layer so the GradCAM\n",
    "        # algorithm cannot be applied\n",
    "        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n",
    "\n",
    "    def compute_heatmap(self, image, eps=1e-8):\n",
    "        \n",
    "        # construct our gradient model by supplying (1) the inputs\n",
    "        # to our pre-trained model, (2) the output of the (presumably)\n",
    "        # final 4D layer in the network, and (3) the output of the\n",
    "        # softmax activations from the model\n",
    "        \n",
    "        gradModel = Model(\n",
    "            inputs=[self.model.inputs],\n",
    "            outputs= [self.model.get_layer(self.layerName).output, self.model.output]\n",
    "        )\n",
    "\n",
    "        # record operations for automatic differentiation\n",
    "        with tf.GradientTape() as tape:\n",
    "            # cast the image tensor to a float-32 data type, pass the\n",
    "            # image through the gradient model, and grab the loss\n",
    "            # associated with the specific class index\n",
    "            inputs = tf.cast(image, tf.float32)\n",
    "            (convOutputs, predictions) = gradModel(inputs)\n",
    "            loss = predictions[:, self.classIdx]\n",
    "\n",
    "        # use automatic differentiation to compute the gradients\n",
    "        grads = tape.gradient(loss, convOutputs)\n",
    "\n",
    "        # compute the guided gradients\n",
    "        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n",
    "        castGrads = tf.cast(grads > 0, \"float32\")\n",
    "        guidedGrads = castConvOutputs * castGrads * grads\n",
    "\n",
    "        # the convolution and guided gradients have a batch dimension\n",
    "        # (which we don't need) so let's grab the volume itself and\n",
    "        # discard the batch\n",
    "        convOutputs = convOutputs[0]\n",
    "        guidedGrads = guidedGrads[0]\n",
    "\n",
    "        # compute the average of the gradient values, and using them\n",
    "        # as weights, compute the ponderation of the filters with\n",
    "        # respect to the weights\n",
    "        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n",
    "        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
    "\n",
    "        # grab the spatial dimensions of the input image and resize\n",
    "        # the output class activation map to match the input image\n",
    "        # dimensions\n",
    "        (w, h) = (image.shape[2], image.shape[1])\n",
    "        heatmap = cv2.resize(cam.numpy(), (w, h))\n",
    "\n",
    "        # normalize the heatmap such that all values lie in the range\n",
    "        # [0, 1], scale the resulting values to the range [0, 255],\n",
    "        # and then convert to an unsigned 8-bit integer\n",
    "        numer = heatmap - np.min(heatmap)\n",
    "        denom = (heatmap.max() - heatmap.min()) + eps\n",
    "        heatmap = numer / denom\n",
    "        heatmap = (heatmap * 255).astype(\"uint8\")\n",
    "\n",
    "        # return the resulting heatmap to the calling function\n",
    "        return heatmap\n",
    "\n",
    "    def overlay_heatmap(self, heatmap, image, alpha=0.5,\n",
    "        colormap=cv2.COLORMAP_JET):\n",
    "        # apply the supplied color map to the heatmap and then\n",
    "        # overlay the heatmap on the input image\n",
    "        heatmap = cv2.applyColorMap(heatmap, colormap)\n",
    "        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n",
    "\n",
    "        # return a 2-tuple of the color mapped heatmap and the output,\n",
    "        # overlaid image\n",
    "        return (heatmap, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0279562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.convolutional.Conv2D at 0x1eb5a0f5520>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded.get_layer('conv2d_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f03e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40458e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "image = load_img(r\"Sutta Reddy/Positive/00001.jpg\", target_size=(227, 227))\n",
    "image = img_to_array(image)\n",
    "image = np.expand_dims(image, axis=0)\n",
    "image /= 255.0\n",
    "\n",
    "preds = model.predict(image)\n",
    "i = np.argmax(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9710abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Positive', 'Negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e782efa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = classes[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73d7989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = GradCAM(model_loaded, i)\n",
    "heatmap = cam.compute_heatmap(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47997cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize the resulting heatmap to the original input image dimensions\n",
    "# and then overlay heatmap on top of the image\n",
    "orig = cv2.imread(\"Sutta Reddy/Positive/00001.jpg\")\n",
    "heatmap = cv2.resize(heatmap, (orig.shape[1], orig.shape[0]))\n",
    "(heatmap, output) = cam.overlay_heatmap(heatmap, orig, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91d4b93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the predicted label on the output image\n",
    "cv2.rectangle(output, (0, 0), (340, 40), (0, 0, 0), -1)\n",
    "cv2.putText(output, label, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "# display the original image and resulting heatmap and output image\n",
    "# to our screen\n",
    "output = np.vstack([orig, heatmap, output])\n",
    "# output = imutils.resize(output, height=700)\n",
    "cv2.imshow(\"Output\", output)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
